{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an object of class PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "lancaster=LancasterStemmer()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deEmoji(instr):\n",
    "    return instr.encode('ascii','ignore').decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "def lemSentence(sentence):\n",
    "    punctuations=\"?:!.,;ï»\"\n",
    "    token_words=word_tokenize(sentence)\n",
    "    stem_sentence=[]\n",
    "    for word in token_words:\n",
    "        if word.isupper():\n",
    "            word = word.lower()\n",
    "        if word in punctuations or len(word)<3:\n",
    "            #token_words.remove(word)\n",
    "            continue\n",
    "        wrd = wordnet_lemmatizer.lemmatize(word, pos=\"v\")\n",
    "        stem_sentence.append(wrd)\n",
    "        stem_sentence.append(\" \")\n",
    "    return \"\".join(stem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "  \n",
    "def putSpace(input): \n",
    "  \n",
    "    # regex [A-Z][a-z]* means any string starting  \n",
    "    # with capital character followed by many  \n",
    "    # lowercase letters  \n",
    "    words = re.findall('[A-Z][a-z]*', input) \n",
    "  \n",
    "    # Change first letter of each word into lower \n",
    "    # case \n",
    "    result = [] \n",
    "    for word in words: \n",
    "        word = chr( ord (word[0]) + 32) + word[1:] \n",
    "        result.append(word) \n",
    "    return ' '.join(result) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = \"weed dope mmemberville hallucinogen stoner stoned cannabis smoke joint ganja crack puff drug high cocaine marijuana stonerfam wakenbake cigar cocaine alcohol pill heroin esctasy tobacco cigarette fuck vodka gin tequila whisky brandy nicotine rehab dopamine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lemSentence(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = words.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = words[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('dsp2.txt','r')\n",
    "data = []\n",
    "for f in file:\n",
    "    data.append(f)\n",
    "hasht = {}\n",
    "for word in words:\n",
    "    hasht[word] = 0\n",
    "for tweet in data:\n",
    "    if type(tweet) != str:\n",
    "        continue\n",
    "    for word in words:\n",
    "        txt = tweet.lower()\n",
    "        c = txt.count(word)\n",
    "        hasht[word] += c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "for i in hasht:\n",
    "    if hasht[i] > 5:\n",
    "        total += hasht[i]\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('weed', 24),\n",
       " ('smoke', 20),\n",
       " ('high', 19),\n",
       " ('stone', 7),\n",
       " ('stoner', 4),\n",
       " ('joint', 4),\n",
       " ('pill', 4),\n",
       " ('fuck', 2),\n",
       " ('mmemberville', 1),\n",
       " ('marijuana', 1),\n",
       " ('alcohol', 1),\n",
       " ('gin', 1),\n",
       " ('dope', 0),\n",
       " ('hallucinogen', 0),\n",
       " ('cannabis', 0),\n",
       " ('ganja', 0),\n",
       " ('crack', 0),\n",
       " ('puff', 0),\n",
       " ('drug', 0),\n",
       " ('cocaine', 0),\n",
       " ('stonerfam', 0),\n",
       " ('wakenbake', 0),\n",
       " ('cigar', 0),\n",
       " ('heroin', 0),\n",
       " ('esctasy', 0),\n",
       " ('tobacco', 0),\n",
       " ('cigarette', 0),\n",
       " ('vodka', 0),\n",
       " ('tequila', 0),\n",
       " ('whisky', 0),\n",
       " ('brandy', 0),\n",
       " ('nicotine', 0),\n",
       " ('rehab', 0),\n",
       " ('dopamine', 0)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(hasht.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
